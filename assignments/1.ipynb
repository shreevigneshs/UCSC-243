{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "https://github.com/shreevigneshs/UCSC-243/blob/main/assignments/1.ipynb",
      "authorship_tag": "ABX9TyOpUqvxaJoT/L6SGvrgfaOd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shreevigneshs/UCSC-243/blob/main/assignments/1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "mKm3rpubTYhl"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "\n",
        "import torch\n",
        "import torchtext\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import Vocab\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en"
      ],
      "metadata": {
        "id": "OffbB19U_TcA",
        "outputId": "1ccfaac5-06a2-4476-e345-db9f45323c0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-10-02 00:02:01.757602: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n",
            "full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.4.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.0/en_core_web_sm-3.4.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8 MB 19.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.4.0) (3.4.1)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.6)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.9.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.10)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.23.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.3.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.8)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.10.1)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.6.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.64.1)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.4.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.4.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (57.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.21.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (8.1.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.11.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.7)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.1.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (21.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (5.2.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.10)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.7.8)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -U nltk"
      ],
      "metadata": {
        "id": "Ki-nfBfV4Pq2"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from nltk.tokenize import tokenizer"
      ],
      "metadata": {
        "id": "P_EsmnzL4RWx"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/NLP243/assignments/1/hw1_train-1.csv\")"
      ],
      "metadata": {
        "id": "U9Tt2FGAYYkC"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df)"
      ],
      "metadata": {
        "id": "bYcwKLslZ1XZ",
        "outputId": "91221097-cfc5-42d0-c3a8-9a45cbdf358a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        ID                                         UTTERANCES  \\\n",
            "0        0               who plays luke on star wars new hope   \n",
            "1        1                     show credits for the godfather   \n",
            "2        2             who was the main actor in the exorcist   \n",
            "3        3  find the female actress from the movie she's t...   \n",
            "4        4                    who played dory on finding nemo   \n",
            "...    ...                                                ...   \n",
            "2307  2307               what was the revenue for toy story 3   \n",
            "2308  2308                                dark knight revenue   \n",
            "2309  2309               how much did the dark night generate   \n",
            "2310  2310                  can i see the lion king's revenue   \n",
            "2311  2311         can i see what the lion king's revenue was   \n",
            "\n",
            "                                     CORE RELATIONS  \n",
            "0     movie.starring.actor movie.starring.character  \n",
            "1                              movie.starring.actor  \n",
            "2                              movie.starring.actor  \n",
            "3                 movie.starring.actor actor.gender  \n",
            "4     movie.starring.actor movie.starring.character  \n",
            "...                                             ...  \n",
            "2307                            movie.gross_revenue  \n",
            "2308                            movie.gross_revenue  \n",
            "2309                            movie.gross_revenue  \n",
            "2310                            movie.gross_revenue  \n",
            "2311                            movie.gross_revenue  \n",
            "\n",
            "[2312 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns = [\"id\", \"text\", \"labels\"]"
      ],
      "metadata": {
        "id": "48Sz4BNSjDo1"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.labels = df.labels.replace(np.nan, \"none\", regex=True)"
      ],
      "metadata": {
        "id": "wUA7YbMjjY9k"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = list(set([_split_label for label in df.labels.unique().flatten() for _split_label in label.split()]))"
      ],
      "metadata": {
        "id": "1Dawr8aFdQwF"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(labels)\n",
        "print(len(labels))"
      ],
      "metadata": {
        "id": "nk4PU_CeoTjh",
        "outputId": "3a0673d2-d05a-445f-f09c-98eeec751eb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['gr.amount', 'movie.subjects', 'movie.estimated_budget', 'movie.locations', 'movie.gross_revenue', 'movie.directed_by', 'none', 'movie.starring.character', 'movie.production_companies', 'movie.produced_by', 'movie.country', 'actor.gender', 'movie.starring.actor', 'movie.initial_release_date', 'movie.genre', 'movie.rating', 'movie.language', 'movie.music', 'person.date_of_birth']\n",
            "19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = list(set([str.lower(_word) for text in df.text for _word in text.split()]))"
      ],
      "metadata": {
        "id": "hAia8EOWeFr5"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokens)"
      ],
      "metadata": {
        "id": "9Z70XbpdeF4B",
        "outputId": "428c8731-b3f1-4148-a42f-e4d06c434bf7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1192"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = [_token[:_token.index(\"'\")] if \"'\" in _token else _token for _token in tokens]"
      ],
      "metadata": {
        "id": "87K-nCzv4tD9"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "table = str.maketrans('', '', string.punctuation)\n",
        "stripped = [token.translate(table) for token in tokens]\n",
        "# remove remaining tokens that are not alphabetic\n",
        "tokens = [token for token in stripped if token.isalpha()]"
      ],
      "metadata": {
        "id": "UwL7eTWw0chF"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = list(set(['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn']))"
      ],
      "metadata": {
        "id": "792eW6QiwVhv"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = [token for token in tokens if not token in stop_words]\n"
      ],
      "metadata": {
        "id": "aM3uHMi15z6q"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokens)"
      ],
      "metadata": {
        "id": "DOxX1R536Cik",
        "outputId": "df66afdc-bce2-474b-bc76-62ec87f05277",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1114"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_2 = [_token for _token in tokens if len(_token) == 2]"
      ],
      "metadata": {
        "id": "iMKPcWn364GR"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_1 = [_token for _token in tokens if len(_token) == 1]"
      ],
      "metadata": {
        "id": "IdHKJp-L7w0D"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(token_1))\n",
        "print(token_1)"
      ],
      "metadata": {
        "id": "rBleH3XR71BK",
        "outputId": "777fefe0-e97a-4aa3-c9e0-0a9cbc9bfb8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n",
            "['b', 'g', 'r', 'e']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(token_2)"
      ],
      "metadata": {
        "id": "qbc7AUIY7Dfq",
        "outputId": "24e0d7dc-dad5-47ab-b0d0-f624e635301c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ju', 'wb', 'jj', 'pi', 'nc', 'ok', 'th', 'pg', 'et', 'iv', 'go', 'ms', 'us', 'fi', 'oz', 'le', 'la', 'de', 'ed', 'yr', 'et', 'uk']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_tokens = Counter(tokens)"
      ],
      "metadata": {
        "id": "I5nC2Yb26PGR"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(all_tokens)"
      ],
      "metadata": {
        "id": "KY3BcY0F6bSU",
        "outputId": "3bcae3dc-2c14-460c-ad46-6f069f4f315a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1061"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "en_tokenizer = get_tokenizer('spacy', language='en')\n",
        "\n",
        "def build_vocab(text, tokenizer):\n",
        "  counter = Counter()\n",
        "  counter.update(tokenizer(text))\n",
        "  return Vocab(counter)\n",
        "\n",
        "en_vocab = build_vocab(\" \".join([text for text in df.text]), en_tokenizer)"
      ],
      "metadata": {
        "id": "ubeVxxfE6Poe",
        "outputId": "87dfc66c-b8fa-4381-971b-771a31368030",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/utils.py:106: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
            "  f'Spacy model \"{language}\" could not be loaded, trying \"{OLD_MODEL_SHORTCUTS[language]}\" instead'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(en_vocab)"
      ],
      "metadata": {
        "id": "r9SUF8vC6Pz-",
        "outputId": "8beba26a-744f-47d5-c2b7-885eb2af9b7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1155"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded = np.zeros(19)\n",
        "encoded[0] = 1.\n",
        "encoded[5] = 1.\n",
        "print(encoded)"
      ],
      "metadata": {
        "id": "3LrnF6FwRVag",
        "outputId": "784dd2b9-7c3b-4060-9dad-a2dd1f9ef109",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MovieDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        path: str,  # Path to the training data csv\n",
        "        vocab_size: int = 100  # How many tokens to include in the vocabulary. Feel free to adjust this!\n",
        "    ):\n",
        "        self.stop_words = list(set(['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn']))\n",
        "        \n",
        "        # Read the csv using pandas read_csv function.\n",
        "        self.data = pd.read_csv(path, index_col=0)\n",
        "\n",
        "        # the given column names are long, so I often rename them for simplicity.\n",
        "        self.data.columns = [\"text\", \"labels\"]\n",
        "\n",
        "        # There's a problem with this data...some of the rows have a label called 'none', and others\n",
        "        # are just empty. These are both referring to the same condition, so lets replace the 'none'\n",
        "        # labels with empty strings to make it easier. Otherwise, you might predict both 'none' and\n",
        "        # another label, which doesn't make any sense.\n",
        "        self.data[\"labels\"] = self.labels.replace(np.nan, \"none\", regex=True)\n",
        "\n",
        "        # For one-hot encoding, we need a list of all unique labels in the dataset and a map between\n",
        "        # labels and a unique ID.\n",
        "        \n",
        "        # TODO create self.labels: a list of every possible label in the dataset\n",
        "        # e.g., ['movie.starring.actor', 'movie.gross_revenue', ...]\n",
        "        # ======================================================================\n",
        "        self.labels = list(set([_split_label for label in df.labels.unique().flatten() for _split_label in label.split()]))\n",
        "        self.n_labels = len(self.labels)\n",
        "        \n",
        "        # TODO create self.label2id: a dictionary which maps the labels from self.labels (above) to a unique integer\n",
        "        # ======================================================================\n",
        "        self.label2id = {}\n",
        "        _cnt = 0\n",
        "        for _label in self.labels:\n",
        "          self.label2id[_label] = _cnt\n",
        "          _cnt += 1\n",
        "        \n",
        "        # Similarly, we often need to make a token vocabulary for encoding the input text. Note that\n",
        "        # this isn't necessary for ALL representations, but for some, you will find it useful. \n",
        "        # However, we are only creating the vocabulary from the training data. What happens if \n",
        "        # the test data has a token we haven't seen before? \n",
        "        # To combat this, we default to a particular token, usually something like <unk> ('unknown').\n",
        "        \n",
        "        # In the future, you will see datasets with hundreds of thousands of unique tokens. Normally,\n",
        "        # we only take the N most common tokens and replace everything else with <unk>. \n",
        "        # Otherwise, our models would be huge! For this dataset, it's not a problem, but you should know\n",
        "        # how to do it. \n",
        "        \n",
        "        # TODO create self.vocab: a dictionary which contains the `vocab_size` most common tokens in the text.\n",
        "        # Here's a hint - check out the `Counter` class from python's `collections` library.\n",
        "        # ======================================================================\n",
        "        \n",
        "        self.vocab = {}\n",
        "\n",
        "        # also, don't forget to include <unk> (unknown)\n",
        "        # TODO assign <unk> a unique ID. \n",
        "        # ======================================================================\n",
        "\n",
        "        _cnt = 0\n",
        "        self.vocab['<unk>'] = 0\n",
        "        _cnt += 1\n",
        "\n",
        "        all_vocab = Counter(list(set([_word for text in self.data.text for _word in self.tokenize(text=text)])))\n",
        "\n",
        "        most_common_vocab = all_vocab.most_common(self.vocab_size-1)\n",
        "\n",
        "        for _token in most_common_vocab.keys():\n",
        "          self.vocab[_token] = _cnt\n",
        "          _cnt += 1\n",
        "\n",
        "        self.vocab_size = _cnt\n",
        "        # self.vocab_size = vocab_size + 1 # plus 1 because <unk>\n",
        "    \n",
        "    def one_hot_encode_labels(self, labels: List[str]):\n",
        "        # For multi-label classification, we're going to one-hot encode our labels.\n",
        "        # This means that instead of having out data be pairs like:\n",
        "        #   {'input': ..., 'output': 2}\n",
        "        # We instead might have multiple correct classes, so we do something like\n",
        "        #   {'input': ..., 'output': [0, 0, 1, 0, ...]}\n",
        "        # where the output is a list with one element per possible label. Then, a 1 in position N means\n",
        "        # the label N is a correct answer.\n",
        "        \n",
        "        # We need to create such a list from the input to this function, `labels`, which is a list\n",
        "        # of labels that appear in a particular example. It might be, for example, \n",
        "        #   ['movie.starring.actor', 'movie.release_date']\n",
        "        # Good thing we have self.label2id! That should help us figure out which \n",
        "        # index corresponds to which label, so we can write our own function. \n",
        "        # Although...this is a very common thing to do in NLP,\n",
        "        # I'm sure it's available in a library somewhere (hint: sklearn). \n",
        "        \n",
        "        # TODO create encoded: a vector (np.array) which is a one-hot encoded \n",
        "        # representation of the input,`labels`. \n",
        "        # ======================================================================\n",
        "\n",
        "        # encoded = labels\n",
        "        encoded = np.zeros(self.n_labels, dtype=torch.int64)\n",
        "        for _label in labels:\n",
        "          encoded[self.label2id[_label]] = 1.\n",
        "        return encoded\n",
        "    \n",
        "    def tokenize(self, text: str):\n",
        "        # Luckily, this dataset is already tokenized; that is, each token is separated by a single\n",
        "        # spce. Normally, text has punctuation, hyphenated words, paragraph breaks, etc.. which\n",
        "        # makes tokenization a more complicated problem. For now, just .split() is good enough.\n",
        "        \n",
        "        tokens = list(set([str.lower(_word) for _word in text.split()]))\n",
        "        tokens = [_token[:_token.index(\"'\")] if \"'\" in _token else _token for _token in tokens]\n",
        "\n",
        "        table = str.maketrans('', '', string.punctuation)\n",
        "        stripped = [token.translate(table) for token in tokens]\n",
        "        tokens = [token for token in tokens if not token in stop_words]\n",
        "        tokens = [token for token in stripped if token.isalpha()]\n",
        "        return tokens\n",
        "    \n",
        "    def encode_tokens(self, tokens: List[str]):\n",
        "        # Think about how you want to encode your tokens. One-hot encoding? Something else \n",
        "        # you've learned in 220 or 243?\n",
        "        # Whatever you decide, it's convenient if you are able to feed the output of this\n",
        "        # function directly into your model, like this:\n",
        "        #   >>> model(encode_tokens(['this', 'is', 'a', 'sentence']))\n",
        "        \n",
        "        # Note: that's only a suggestion, you don't have to feed this directly into the model.\n",
        "        # Feel free to set up your data/model pipeline as you see fit. \n",
        "        \n",
        "        # TODO create encoded: an encoded representation of `tokens`.\n",
        "        # ======================================================================\n",
        "\n",
        "        # encoded = tokens\n",
        "        token_ids = [self.vocab[_token] if _token in self.vocab else self.vocab[\"<unk>\"] for _token in tokens]\n",
        "        t = torch.tensor(token_ids, dtype = torch.float)\n",
        "        one_hot_vector = F.one_hot(x = t, num_classes=self.vocab_size)\n",
        "        return encoded\n",
        "\n",
        "    def __len__(self):\n",
        "        # PyTorch expects every Dataset class to implement the __len__ function. \n",
        "        # Most of the time, it's very simple like this.\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, n: int):\n",
        "        # TODO get the nth item of your data, and process it so it's ready to used in your model\n",
        "        # and for training.\n",
        "        # ======================================================================\n",
        "        \n",
        "        # Make sure the output of this function is either an np.array, a\n",
        "        # torch.Tensor, or a tuple of several of these. That way, pytorch\n",
        "        # can combine them into batches properly using its default collate_fn in the DataLoader.\n",
        "        # If you're using nn.Embedding, you will have to deal with padding,\n",
        "        # but I'll leave that for you :)\n",
        "\n",
        "        text = self.data.texts[n]\n",
        "        labels = self.data.labels[n]\n",
        "        return self.encode_tokens(text), self.one_hot_encode_labels(labels)\n",
        "\n"
      ],
      "metadata": {
        "id": "VKAZWO-0eGJ1"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PR3QwnL8IhER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "xd9WJcjEIuWJ"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiLayerPerceptron(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, output_size, dropout=False, dropout_p=0.1):\n",
        "        super(MultiLayerPerceptron, self).__init__()\n",
        "        self.input_layer = nn.Linear(input_size, hidden_size, bias=True)\n",
        "        self.hidden_layer = nn.Linear(hidden_size, output_size, bias=True)\n",
        "\n",
        "        self.add_dropout = dropout\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = F.relu(self.input_layer(x))\n",
        "        if self.add_dropout:\n",
        "            logits = self.hidden_layer(self.dropout(h))\n",
        "        else:\n",
        "            logits = self.hidden_layer(h)\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "r93dh2ViIhzE"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiClassTrainer(object):\n",
        "    \"\"\"\n",
        "    Trainer for training a multi-class classification model\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, optimizer, loss_fn, device=\"cpu\", log_every_n=None):\n",
        "        self.model = model.to(device)\n",
        "        self.optimizer = optimizer\n",
        "        self.device = device\n",
        "        self.loss_fn = loss_fn\n",
        "        \n",
        "        self.log_every_n = log_every_n if log_every_n else 0\n",
        "\n",
        "\n",
        "    def _print_summary(self):\n",
        "        print(self.model)\n",
        "        print(self.optimizer)\n",
        "        print(self.loss_fn)\n",
        "\n",
        "    def train(self, loader):\n",
        "        \"\"\"\n",
        "        Run a single epoch of training\n",
        "        \"\"\"\n",
        "\n",
        "        self.model.train() # Run model in training mode\n",
        "\n",
        "        loss_history = []\n",
        "        running_loss = 0.\n",
        "        running_loss_history = []\n",
        "\n",
        "        for i, batch in tqdm(enumerate(loader)):\n",
        "            batch_size = batch[0].shape[0]\n",
        "            self.optimizer.zero_grad() # Always set gradient to 0 before computing it\n",
        "\n",
        "            logits = self.model(batch[0].to(self.device)) # __call__ model() in this case: __call__ internally calls forward()\n",
        "            # [batch_size, num_classes]\n",
        "\n",
        "            loss = self.loss_fn(logits, batch[1].view(-1).to(self.device)) # Compute loss: Cross entropy loss\n",
        "\n",
        "            loss_history.append(loss.item())\n",
        "\n",
        "            \n",
        "\n",
        "            running_loss += (loss_history[-1] - running_loss) / (i + 1) # Compute rolling average\n",
        "\n",
        "            if self.log_every_n and i % self.log_every_n == 0:\n",
        "                print(\"Running loss: \", running_loss)\n",
        "\n",
        "            running_loss_history.append(running_loss)\n",
        "\n",
        "            loss.backward() # Perform backprop, which will compute dL/dw\n",
        "\n",
        "            nn.utils.clip_grad_norm_(self.model.parameters(), 3.0) # We clip gradient's norm to 3\n",
        "\n",
        "            self.optimizer.step() # Update step: w = w - eta * dL / dW : eta = 1e-2 (0.01), gradient = 5e30; update value of 5e28\n",
        "\n",
        "        print(\"Epoch completed!\")\n",
        "        print(\"Epoch Loss: \", running_loss)\n",
        "        print(\"Epoch Perplexity: \", math.exp(running_loss))\n",
        "\n",
        "        # The history information can allow us to draw a loss plot\n",
        "        return loss_history, running_loss_history\n",
        "\n",
        "    def evaluate(self, loader, labels):\n",
        "        \"\"\"\n",
        "        Evaluate the model on a validation set\n",
        "        \"\"\"\n",
        "\n",
        "        self.model.eval() # Run model in eval mode (disables dropout layer)\n",
        "\n",
        "        batch_wise_true_labels = []\n",
        "        batch_wise_predictions = []\n",
        "\n",
        "        loss_history = []\n",
        "        running_loss = 0.\n",
        "        running_loss_history = []\n",
        "\n",
        "        with torch.no_grad(): # Disable gradient computation - required only during training\n",
        "            for i, batch in tqdm(enumerate(loader)):\n",
        "                # batch[0] shape: (batch_size, input_size)\n",
        "\n",
        "                logits = self.model(batch[0].to(self.device)) # Run forward pass (except we don't store gradients)\n",
        "                # logits shape: (batch_size, num_classes)\n",
        "                \n",
        "                loss = self.loss_fn(logits, batch[1].view(-1).to(self.device)) # Compute loss\n",
        "                # No backprop is done during validation\n",
        "                \n",
        "                # Instead of using CrossEntropyLoss, you use BCEWithLogitsLoss\n",
        "                # BCEWithLogitsLoss - independently calculates loss for each class\n",
        "                \n",
        "\n",
        "                loss_history.append(loss.item())\n",
        "\n",
        "                running_loss += (loss_history[-1] - running_loss) / (i + 1) # Compute rolling average\n",
        "                \n",
        "                running_loss_history.append(running_loss)\n",
        "\n",
        "                # logits : [batch_size, num_classes] and each of the values in logits can be anything (-infinity, +infity)\n",
        "                # Converts the raw outputs into probabilities for each class using softmax\n",
        "                probs = F.softmax(logits, dim=-1) \n",
        "                # probs shape: (batch_size, num_classes)\n",
        "                # -1 dimension picks the last dimension in the shape of the tensor, in this case 'num_classes'\n",
        "                \n",
        "\n",
        "                # softmax vector: [[0.1, 0.2, 0.6, 0.1, 0.0], [0.9, 0.01, 0.01, 0.01, 0.07]]\n",
        "                # output tensor: [2, 0]\n",
        "                predictions = torch.argmax(probs, dim=-1) # Output predictions; Argmax picks the index with the highest probability among all the classes (choosing our most probable class)\n",
        "                # predictions shape: (batch_size)\n",
        "\n",
        "                batch_wise_true_labels.append(batch[1].tolist())\n",
        "                batch_wise_predictions.append(predictions.tolist())\n",
        "        \n",
        "        # flatten the list of predictions using itertools\n",
        "        all_true_labels = list(chain.from_iterable(batch_wise_true_labels))\n",
        "        all_predictions = list(chain.from_iterable(batch_wise_predictions))\n",
        "\n",
        "        # Now we can generate a classification report\n",
        "        print(\"Classification report after epoch:\")\n",
        "        print(classification_report(all_true_labels, all_predictions, target_names=labels))\n",
        "\n",
        "        return loss_history, running_loss_history\n",
        "\n",
        "    def get_model_dict(self):\n",
        "        return self.model.state_dict()\n",
        "\n",
        "    def run_training(self, train_loader, valid_loader, labels, n_epochs=10):\n",
        "        # Useful for us to review what experiment we're running\n",
        "        # Normally, you'd want to save this to a file\n",
        "        self._print_summary()\n",
        "\n",
        "        train_losses = []\n",
        "        train_running_losses = []\n",
        "\n",
        "        valid_losses = []\n",
        "        valid_running_losses = []\n",
        "\n",
        "        for i in range(n_epochs):\n",
        "            loss_history, running_loss_history = self.train(train_loader)\n",
        "            valid_loss_history, valid_running_loss_history = self.evaluate(valid_loader, labels)\n",
        "\n",
        "            train_losses.append(loss_history)\n",
        "            train_running_losses.append(running_loss_history)\n",
        "\n",
        "            valid_losses.append(valid_loss_history)\n",
        "            valid_running_losses.append(valid_running_loss_history)\n",
        "\n",
        "        # Training done, let's look at the loss curves\n",
        "        all_train_losses = list(chain.from_iterable(train_losses))\n",
        "        all_train_running_losses = list(chain.from_iterable(train_running_losses))\n",
        "\n",
        "        all_valid_losses = list(chain.from_iterable(valid_losses))\n",
        "        all_valid_running_losses = list(chain.from_iterable(valid_running_losses))\n",
        "\n",
        "        train_epoch_idx = range(len(all_train_losses))\n",
        "        valid_epoch_idx = range(len(all_valid_losses))\n",
        "        # sns.lineplot(epoch_idx, all_losses)\n",
        "        sns.lineplot(train_epoch_idx, all_train_running_losses)\n",
        "        sns.lineplot(valid_epoch_idx, all_valid_running_losses)\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "6gwDC4mCKBkI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}