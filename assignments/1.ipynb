{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "https://github.com/shreevigneshs/UCSC-243/blob/main/assignments/1.ipynb",
      "authorship_tag": "ABX9TyP5s1pS1Bde50DxeiwFMDjM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shreevigneshs/UCSC-243/blob/main/assignments/1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mKm3rpubTYhl"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "\n",
        "import torch\n",
        "import torchtext\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import Vocab\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !python -m spacy download en"
      ],
      "metadata": {
        "id": "OffbB19U_TcA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -U nltk"
      ],
      "metadata": {
        "id": "Ki-nfBfV4Pq2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from nltk.tokenize import tokenizer"
      ],
      "metadata": {
        "id": "P_EsmnzL4RWx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/NLP243/assignments/1/hw1_train-1.csv\")"
      ],
      "metadata": {
        "id": "U9Tt2FGAYYkC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYcwKLslZ1XZ",
        "outputId": "b175c9d4-877c-4bee-9542-91c227eb3d2f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        ID                                         UTTERANCES  \\\n",
            "0        0               who plays luke on star wars new hope   \n",
            "1        1                     show credits for the godfather   \n",
            "2        2             who was the main actor in the exorcist   \n",
            "3        3  find the female actress from the movie she's t...   \n",
            "4        4                    who played dory on finding nemo   \n",
            "...    ...                                                ...   \n",
            "2307  2307               what was the revenue for toy story 3   \n",
            "2308  2308                                dark knight revenue   \n",
            "2309  2309               how much did the dark night generate   \n",
            "2310  2310                  can i see the lion king's revenue   \n",
            "2311  2311         can i see what the lion king's revenue was   \n",
            "\n",
            "                                     CORE RELATIONS  \n",
            "0     movie.starring.actor movie.starring.character  \n",
            "1                              movie.starring.actor  \n",
            "2                              movie.starring.actor  \n",
            "3                 movie.starring.actor actor.gender  \n",
            "4     movie.starring.actor movie.starring.character  \n",
            "...                                             ...  \n",
            "2307                            movie.gross_revenue  \n",
            "2308                            movie.gross_revenue  \n",
            "2309                            movie.gross_revenue  \n",
            "2310                            movie.gross_revenue  \n",
            "2311                            movie.gross_revenue  \n",
            "\n",
            "[2312 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns = [\"id\", \"text\", \"labels\"]"
      ],
      "metadata": {
        "id": "48Sz4BNSjDo1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.labels = df.labels.replace(np.nan, \"none\", regex=True)"
      ],
      "metadata": {
        "id": "wUA7YbMjjY9k"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = list(set([_split_label for label in df.labels.unique().flatten() for _split_label in label.split()]))"
      ],
      "metadata": {
        "id": "1Dawr8aFdQwF"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(labels)\n",
        "print(len(labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nk4PU_CeoTjh",
        "outputId": "8508e3a9-4727-455c-b66e-110ef6509679"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['movie.estimated_budget', 'movie.production_companies', 'gr.amount', 'movie.subjects', 'movie.gross_revenue', 'movie.language', 'movie.locations', 'movie.starring.actor', 'movie.starring.character', 'movie.initial_release_date', 'person.date_of_birth', 'movie.genre', 'none', 'actor.gender', 'movie.produced_by', 'movie.country', 'movie.rating', 'movie.directed_by', 'movie.music']\n",
            "19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = list(set([str.lower(_word) for text in df.text for _word in text.split()]))"
      ],
      "metadata": {
        "id": "hAia8EOWeFr5"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Z70XbpdeF4B",
        "outputId": "11789695-9787-4f36-ed2b-da8e39b50e65"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1192"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = [_token[:_token.index(\"'\")] if \"'\" in _token else _token for _token in tokens]"
      ],
      "metadata": {
        "id": "87K-nCzv4tD9"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "table = str.maketrans('', '', string.punctuation)\n",
        "stripped = [token.translate(table) for token in tokens]\n",
        "# remove remaining tokens that are not alphabetic\n",
        "tokens = [token for token in stripped if token.isalpha()]"
      ],
      "metadata": {
        "id": "UwL7eTWw0chF"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = list(set(['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn']))"
      ],
      "metadata": {
        "id": "792eW6QiwVhv"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = [token for token in tokens if not token in stop_words]\n"
      ],
      "metadata": {
        "id": "aM3uHMi15z6q"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOxX1R536Cik",
        "outputId": "82bbac84-e7df-4092-933b-72557349c8f2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1096"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_2 = [_token for _token in tokens if len(_token) == 2]"
      ],
      "metadata": {
        "id": "iMKPcWn364GR"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_1 = [_token for _token in tokens if len(_token) == 1]"
      ],
      "metadata": {
        "id": "IdHKJp-L7w0D"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(token_1))\n",
        "print(token_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBleH3XR71BK",
        "outputId": "355e3c94-d472-4f67-c12a-000d46df3f70"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n",
            "['b', 'r', 'g', 'e']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(token_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbc7AUIY7Dfq",
        "outputId": "08935c2c-51b9-4b61-9a40-ebb7229a02f3"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['yr', 'ok', 'iv', 'de', 'et', 'ju', 'uk', 'le', 'pi', 'pg', 'wb', 'ms', 'la', 'ed', 'th', 'go', 'nc', 'et', 'fi', 'oz', 'jj', 'us']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_tokens = Counter(tokens)"
      ],
      "metadata": {
        "id": "I5nC2Yb26PGR"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(all_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KY3BcY0F6bSU",
        "outputId": "3a495659-0c38-4cca-eef5-dd2bf990222a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1061"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "en_tokenizer = get_tokenizer('spacy', language='en')\n",
        "\n",
        "def build_vocab(text, tokenizer):\n",
        "  counter = Counter()\n",
        "  counter.update(tokenizer(text))\n",
        "  return Vocab(counter)\n",
        "\n",
        "en_vocab = build_vocab(\" \".join([text for text in df.text]), en_tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubeVxxfE6Poe",
        "outputId": "4591a77f-e253-4b47-b6c6-9400006667b2"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/utils.py:106: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
            "  f'Spacy model \"{language}\" could not be loaded, trying \"{OLD_MODEL_SHORTCUTS[language]}\" instead'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(en_vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9SUF8vC6Pz-",
        "outputId": "4aaf8956-72ee-4457-bfe0-d02d1a2cd06a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1155"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t = torch.tensor([6,6,7,8,6,1,7], dtype = torch.int64)\n",
        "one_hot_vector = F.one_hot(x = t, num_classes=9)\n",
        "print(one_hot_vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LrnF6FwRVag",
        "outputId": "6d8f1cc7-f81d-4946-a254-830792577b62"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
            "        [0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 1, 0]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b = torch.zeros(7, 9, dtype=torch.int64)"
      ],
      "metadata": {
        "id": "9s9LOOkcypR0"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c = torch.cat((one_hot_vector, b), 0)\n",
        "print(c)"
      ],
      "metadata": {
        "id": "jJv7ZQYiy4YR",
        "outputId": "827a5ce2-8b17-4906-ee2e-6da028ec2838",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
            "        [0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from numpy import argmax\n",
        "# define the text string\n",
        "data = 'hello'\n",
        "# define universe of possible input values\n",
        "alphabet = 'abcdefghijklmnopqrstuvwxyz '\n",
        "# define a mapping of chars to integers\n",
        "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
        "int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
        "# integer encode input data\n",
        "integer_encoded = [char_to_int[char] for char in data]\n",
        "print(integer_encoded)\n",
        "# convert tensors into one-hot encoding \n",
        "torch.nn.functional.one_hot(torch.tensor(integer_encoded))"
      ],
      "metadata": {
        "id": "EenhMtvZ-pTk",
        "outputId": "cd89a66c-2ee3-4abb-8a9a-56a09268e15a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7, 4, 11, 11, 14]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.functional import norm\n",
        "class MovieDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        path: str,  # Path to the training data csv\n",
        "        vocab_size: int = 100  # How many tokens to include in the vocabulary. Feel free to adjust this!\n",
        "    ):\n",
        "        self.input_text_len = 50\n",
        "        self.stop_words = list(set(['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn']))\n",
        "        \n",
        "        # Read the csv using pandas read_csv function.\n",
        "        self.data = pd.read_csv(path, index_col=0)\n",
        "\n",
        "        # the given column names are long, so I often rename them for simplicity.\n",
        "        self.data.columns = [\"text\", \"labels\"]\n",
        "\n",
        "        # There's a problem with this data...some of the rows have a label called 'none', and others\n",
        "        # are just empty. These are both referring to the same condition, so lets replace the 'none'\n",
        "        # labels with empty strings to make it easier. Otherwise, you might predict both 'none' and\n",
        "        # another label, which doesn't make any sense.\n",
        "        self.data[\"labels\"] = self.data.labels.replace(np.nan, \"none\", regex=True)\n",
        "\n",
        "        # For one-hot encoding, we need a list of all unique labels in the dataset and a map between\n",
        "        # labels and a unique ID.\n",
        "        \n",
        "        # TODO create self.labels: a list of every possible label in the dataset\n",
        "        # e.g., ['movie.starring.actor', 'movie.gross_revenue', ...]\n",
        "        # ======================================================================\n",
        "        self.labels = list(set([_split_label for label in df.labels.unique().flatten() for _split_label in label.split()]))\n",
        "        self.n_labels = len(self.labels)\n",
        "        \n",
        "        # TODO create self.label2id: a dictionary which maps the labels from self.labels (above) to a unique integer\n",
        "        # ======================================================================\n",
        "        self.label2id = {}\n",
        "        _cnt = 0\n",
        "        for _label in self.labels:\n",
        "          self.label2id[_label] = _cnt\n",
        "          _cnt += 1\n",
        "        \n",
        "        # Similarly, we often need to make a token vocabulary for encoding the input text. Note that\n",
        "        # this isn't necessary for ALL representations, but for some, you will find it useful. \n",
        "        # However, we are only creating the vocabulary from the training data. What happens if \n",
        "        # the test data has a token we haven't seen before? \n",
        "        # To combat this, we default to a particular token, usually something like <unk> ('unknown').\n",
        "        \n",
        "        # In the future, you will see datasets with hundreds of thousands of unique tokens. Normally,\n",
        "        # we only take the N most common tokens and replace everything else with <unk>. \n",
        "        # Otherwise, our models would be huge! For this dataset, it's not a problem, but you should know\n",
        "        # how to do it. \n",
        "        \n",
        "        # TODO create self.vocab: a dictionary which contains the `vocab_size` most common tokens in the text.\n",
        "        # Here's a hint - check out the `Counter` class from python's `collections` library.\n",
        "        # ======================================================================\n",
        "        \n",
        "        self.vocab = {}\n",
        "\n",
        "        # also, don't forget to include <unk> (unknown)\n",
        "        # TODO assign <unk> a unique ID. \n",
        "        # ======================================================================\n",
        "\n",
        "        _cnt = 0\n",
        "        self.vocab['<unk>'] = 0\n",
        "        _cnt += 1\n",
        "\n",
        "        all_vocab = Counter(list(set([_word for text in self.data.text for _word in self.tokenize(text=text)])))\n",
        "\n",
        "        most_common_vocab = all_vocab.most_common(vocab_size-1)\n",
        "\n",
        "        for _x in most_common_vocab:\n",
        "          self.vocab[_x[0]] = _cnt\n",
        "          _cnt += 1\n",
        "\n",
        "        self.vocab_size = _cnt\n",
        "        # self.vocab_size = vocab_size + 1 # plus 1 because <unk>\n",
        "    \n",
        "    def one_hot_encode_labels(self, label: str):\n",
        "        # For multi-label classification, we're going to one-hot encode our labels.\n",
        "        # This means that instead of having out data be pairs like:\n",
        "        #   {'input': ..., 'output': 2}\n",
        "        # We instead might have multiple correct classes, so we do something like\n",
        "        #   {'input': ..., 'output': [0, 0, 1, 0, ...]}\n",
        "        # where the output is a list with one element per possible label. Then, a 1 in position N means\n",
        "        # the label N is a correct answer.\n",
        "        \n",
        "        # We need to create such a list from the input to this function, `labels`, which is a list\n",
        "        # of labels that appear in a particular example. It might be, for example, \n",
        "        #   ['movie.starring.actor', 'movie.release_date']\n",
        "        # Good thing we have self.label2id! That should help us figure out which \n",
        "        # index corresponds to which label, so we can write our own function. \n",
        "        # Although...this is a very common thing to do in NLP,\n",
        "        # I'm sure it's available in a library somewhere (hint: sklearn). \n",
        "        \n",
        "        # TODO create encoded: a vector (np.array) which is a one-hot encoded \n",
        "        # representation of the input,`labels`. \n",
        "        # ======================================================================\n",
        "\n",
        "        # encoded = labels\n",
        "        encoded = np.zeros(self.n_labels, dtype=int)\n",
        "        normalized_label = list(set([_split_label for _split_label in label.split()]))\n",
        "        for _label in normalized_label:\n",
        "          encoded[self.label2id[_label]] = 1.\n",
        "        return encoded\n",
        "    \n",
        "    def tokenize(self, text: str):\n",
        "        # Luckily, this dataset is already tokenized; that is, each token is separated by a single\n",
        "        # spce. Normally, text has punctuation, hyphenated words, paragraph breaks, etc.. which\n",
        "        # makes tokenization a more complicated problem. For now, just .split() is good enough.\n",
        "        \n",
        "        tokens = list(set([str.lower(_word) for _word in text.split()]))\n",
        "        tokens = [_token[:_token.index(\"'\")] if \"'\" in _token else _token for _token in tokens]\n",
        "\n",
        "        table = str.maketrans('', '', string.punctuation)\n",
        "        stripped = [token.translate(table) for token in tokens]\n",
        "        tokens = [token for token in tokens if not token in stop_words]\n",
        "        tokens = [token for token in stripped if token.isalpha()]\n",
        "        return tokens\n",
        "    \n",
        "    def encode_tokens(self, tokens: List[str]):\n",
        "        # Think about how you want to encode your tokens. One-hot encoding? Something else \n",
        "        # you've learned in 220 or 243?\n",
        "        # Whatever you decide, it's convenient if you are able to feed the output of this\n",
        "        # function directly into your model, like this:\n",
        "        #   >>> model(encode_tokens(['this', 'is', 'a', 'sentence']))\n",
        "        \n",
        "        # Note: that's only a suggestion, you don't have to feed this directly into the model.\n",
        "        # Feel free to set up your data/model pipeline as you see fit. \n",
        "        \n",
        "        # TODO create encoded: an encoded representation of `tokens`.\n",
        "        # ======================================================================\n",
        "\n",
        "        # encoded = tokens\n",
        "        token_ids = [self.vocab[_token] if _token in self.vocab else self.vocab[\"<unk>\"] for _token in tokens]\n",
        "        if self.input_text_len < len(token_ids):\n",
        "          token_ids = token_ids[:self.input_text_len]\n",
        "        t = torch.tensor(token_ids, dtype = torch.int64)\n",
        "        one_hot_vector = F.one_hot(x = t, num_classes=self.vocab_size)\n",
        "        if self.input_text_len > len(token_ids):\n",
        "          padding = torch.zeros(self.input_text_len - len(token_ids), self.vocab_size, dtype=torch.int64)\n",
        "          encoded = torch.cat((one_hot_vector, padding), 0)\n",
        "        else:\n",
        "          encoded = one_hot_vector\n",
        "        return encoded\n",
        "\n",
        "    def __len__(self):\n",
        "        # PyTorch expects every Dataset class to implement the __len__ function. \n",
        "        # Most of the time, it's very simple like this.\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, n: int):\n",
        "        # TODO get the nth item of your data, and process it so it's ready to used in your model\n",
        "        # and for training.\n",
        "        # ======================================================================\n",
        "        \n",
        "        # Make sure the output of this function is either an np.array, a\n",
        "        # torch.Tensor, or a tuple of several of these. That way, pytorch\n",
        "        # can combine them into batches properly using its default collate_fn in the DataLoader.\n",
        "        # If you're using nn.Embedding, you will have to deal with padding,\n",
        "        # but I'll leave that for you :)\n",
        "\n",
        "        text = self.data.text[n]\n",
        "        labels = self.data.labels[n]\n",
        "        return self.encode_tokens(text).to(torch.float), self.one_hot_encode_labels(labels).astype(float)\n",
        "\n"
      ],
      "metadata": {
        "id": "VKAZWO-0eGJ1"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the data; you'll need to upload this file, or download the notebook and run it locally if you want this to work. \n",
        "dataset = MovieDataset(\"/content/drive/MyDrive/NLP243/assignments/1/hw1_train-1.csv\")\n",
        "\n",
        "# A small batch size of 2 makes it easier to debug for printing. \n",
        "data_loader = DataLoader(dataset, batch_size=2)"
      ],
      "metadata": {
        "id": "PR3QwnL8IhER"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Zipping the dataloader with range(N) lets us only print the first N batches\n",
        "for _, batch in zip(range(5), data_loader):\n",
        "    # Do something here; maybe print the batch to see if it looks right to you?\n",
        "    print(batch[0].shape, batch[1].shape)"
      ],
      "metadata": {
        "id": "Ydrp8Z3Bj_BW",
        "outputId": "aaeb1a53-9e4d-42ed-da1d-1723360d4cb3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 50, 100]) torch.Size([2, 19])\n",
            "torch.Size([2, 50, 100]) torch.Size([2, 19])\n",
            "torch.Size([2, 50, 100]) torch.Size([2, 19])\n",
            "torch.Size([2, 50, 100]) torch.Size([2, 19])\n",
            "torch.Size([2, 50, 100]) torch.Size([2, 19])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "def __init__(self, vocab_size=100, input_size=50, hidden_size=10, output_size=19, dropout=False, dropout_p=0.1):\n",
        "        super(MLP, self).__init__()\n",
        "        self.input_layer = nn.Linear(vocab_sizeembedding_dim)\n",
        "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
        "        self.linear2 = nn.Linear(128, vocab_size)\n",
        "def forward(self, inputs):\n",
        "        embeds = self.embeddings(inputs).view((1, -1))\n",
        "        out = F.relu(self.linear1(embeds))\n",
        "        out = self.linear2(out)\n",
        "        log_probs = F.log_softmax(out, dim=1)\n",
        "        return log_probs"
      ],
      "metadata": {
        "id": "DXe00SnIBAjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I_M1GdPKBAW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TjM_4w1ABAKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ixt3bDUTA__e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EceIUOTOA_mG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiLayerPerceptron(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size=50, hidden_size=100, output_size=19, dropout=False, dropout_p=0.1):\n",
        "        super(MultiLayerPerceptron, self).__init__()\n",
        "        self.input_layer = nn.Linear(input_size, hidden_size, bias=True)\n",
        "        self.hidden_layer = nn.Linear(hidden_size, output_size, bias=True)\n",
        "\n",
        "        self.add_dropout = dropout\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = F.relu(self.input_layer(x).view(1, -1))\n",
        "        if self.add_dropout:\n",
        "            logits = self.hidden_layer(self.dropout(h))\n",
        "        else:\n",
        "            logits = self.hidden_layer(h)\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "r93dh2ViIhzE"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(MultiLayerPerceptron())"
      ],
      "metadata": {
        "id": "IgsWOO5j91qA",
        "outputId": "b8ceace4-8c74-4817-e622-162cd36aaf73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MultiLayerPerceptron(\n",
            "  (input_layer): Linear(in_features=50, out_features=100, bias=True)\n",
            "  (hidden_layer): Linear(in_features=100, out_features=19, bias=True)\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiClassTrainer(object):\n",
        "    \"\"\"\n",
        "    Trainer for training a multi-class classification model\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, optimizer, loss_fn, device=\"cpu\", log_every_n=None):\n",
        "        self.model = model.to(device)\n",
        "        self.optimizer = optimizer\n",
        "        self.device = device\n",
        "        self.loss_fn = loss_fn\n",
        "        \n",
        "        self.log_every_n = log_every_n if log_every_n else 0\n",
        "\n",
        "\n",
        "    def _print_summary(self):\n",
        "        print(self.model)\n",
        "        print(self.optimizer)\n",
        "        print(self.loss_fn)\n",
        "\n",
        "    def train(self, loader):\n",
        "        \"\"\"\n",
        "        Run a single epoch of training\n",
        "        \"\"\"\n",
        "\n",
        "        self.model.train() # Run model in training mode\n",
        "\n",
        "        loss_history = []\n",
        "        running_loss = 0.\n",
        "        running_loss_history = []\n",
        "\n",
        "        for i, batch in tqdm(enumerate(loader)):\n",
        "            batch_size = batch[0].shape[0]\n",
        "            self.optimizer.zero_grad() # Always set gradient to 0 before computing it\n",
        "\n",
        "            logits = self.model(batch[0].to(self.device)) # __call__ model() in this case: __call__ internally calls forward()\n",
        "            # [batch_size, num_classes]\n",
        "\n",
        "            loss = self.loss_fn(logits, batch[1].view(-1).to(self.device)) # Compute loss: Cross entropy loss\n",
        "\n",
        "            loss_history.append(loss.item())\n",
        "\n",
        "            \n",
        "\n",
        "            running_loss += (loss_history[-1] - running_loss) / (i + 1) # Compute rolling average\n",
        "\n",
        "            if self.log_every_n and i % self.log_every_n == 0:\n",
        "                print(\"Running loss: \", running_loss)\n",
        "\n",
        "            running_loss_history.append(running_loss)\n",
        "\n",
        "            loss.backward() # Perform backprop, which will compute dL/dw\n",
        "\n",
        "            nn.utils.clip_grad_norm_(self.model.parameters(), 3.0) # We clip gradient's norm to 3\n",
        "\n",
        "            self.optimizer.step() # Update step: w = w - eta * dL / dW : eta = 1e-2 (0.01), gradient = 5e30; update value of 5e28\n",
        "\n",
        "        print(\"Epoch completed!\")\n",
        "        print(\"Epoch Loss: \", running_loss)\n",
        "        print(\"Epoch Perplexity: \", math.exp(running_loss))\n",
        "\n",
        "        # The history information can allow us to draw a loss plot\n",
        "        return loss_history, running_loss_history\n",
        "\n",
        "    def evaluate(self, loader, labels):\n",
        "        \"\"\"\n",
        "        Evaluate the model on a validation set\n",
        "        \"\"\"\n",
        "\n",
        "        self.model.eval() # Run model in eval mode (disables dropout layer)\n",
        "\n",
        "        batch_wise_true_labels = []\n",
        "        batch_wise_predictions = []\n",
        "\n",
        "        loss_history = []\n",
        "        running_loss = 0.\n",
        "        running_loss_history = []\n",
        "\n",
        "        with torch.no_grad(): # Disable gradient computation - required only during training\n",
        "            for i, batch in tqdm(enumerate(loader)):\n",
        "                # batch[0] shape: (batch_size, input_size)\n",
        "\n",
        "                logits = self.model(batch[0].to(self.device)) # Run forward pass (except we don't store gradients)\n",
        "                # logits shape: (batch_size, num_classes)\n",
        "                \n",
        "                loss = self.loss_fn(logits, batch[1].view(-1).to(self.device)) # Compute loss\n",
        "                # No backprop is done during validation\n",
        "                \n",
        "                # Instead of using CrossEntropyLoss, you use BCEWithLogitsLoss\n",
        "                # BCEWithLogitsLoss - independently calculates loss for each class\n",
        "                \n",
        "\n",
        "                loss_history.append(loss.item())\n",
        "\n",
        "                running_loss += (loss_history[-1] - running_loss) / (i + 1) # Compute rolling average\n",
        "                \n",
        "                running_loss_history.append(running_loss)\n",
        "\n",
        "                # logits : [batch_size, num_classes] and each of the values in logits can be anything (-infinity, +infity)\n",
        "                # Converts the raw outputs into probabilities for each class using softmax\n",
        "                probs = F.softmax(logits, dim=-1) \n",
        "                # probs shape: (batch_size, num_classes)\n",
        "                # -1 dimension picks the last dimension in the shape of the tensor, in this case 'num_classes'\n",
        "                \n",
        "\n",
        "                # softmax vector: [[0.1, 0.2, 0.6, 0.1, 0.0], [0.9, 0.01, 0.01, 0.01, 0.07]]\n",
        "                # output tensor: [2, 0]\n",
        "                predictions = torch.argmax(probs, dim=-1) # Output predictions; Argmax picks the index with the highest probability among all the classes (choosing our most probable class)\n",
        "                # predictions shape: (batch_size)\n",
        "\n",
        "                batch_wise_true_labels.append(batch[1].tolist())\n",
        "                batch_wise_predictions.append(predictions.tolist())\n",
        "        \n",
        "        # flatten the list of predictions using itertools\n",
        "        all_true_labels = list(chain.from_iterable(batch_wise_true_labels))\n",
        "        all_predictions = list(chain.from_iterable(batch_wise_predictions))\n",
        "\n",
        "        # Now we can generate a classification report\n",
        "        print(\"Classification report after epoch:\")\n",
        "        print(classification_report(all_true_labels, all_predictions, target_names=labels))\n",
        "\n",
        "        return loss_history, running_loss_history\n",
        "\n",
        "    def get_model_dict(self):\n",
        "        return self.model.state_dict()\n",
        "\n",
        "    def run_training(self, train_loader, valid_loader, labels, n_epochs=10):\n",
        "        # Useful for us to review what experiment we're running\n",
        "        # Normally, you'd want to save this to a file\n",
        "        self._print_summary()\n",
        "\n",
        "        train_losses = []\n",
        "        train_running_losses = []\n",
        "\n",
        "        valid_losses = []\n",
        "        valid_running_losses = []\n",
        "\n",
        "        for i in range(n_epochs):\n",
        "            loss_history, running_loss_history = self.train(train_loader)\n",
        "            valid_loss_history, valid_running_loss_history = self.evaluate(valid_loader, labels)\n",
        "\n",
        "            train_losses.append(loss_history)\n",
        "            train_running_losses.append(running_loss_history)\n",
        "\n",
        "            valid_losses.append(valid_loss_history)\n",
        "            valid_running_losses.append(valid_running_loss_history)\n",
        "\n",
        "        # Training done, let's look at the loss curves\n",
        "        all_train_losses = list(chain.from_iterable(train_losses))\n",
        "        all_train_running_losses = list(chain.from_iterable(train_running_losses))\n",
        "\n",
        "        all_valid_losses = list(chain.from_iterable(valid_losses))\n",
        "        all_valid_running_losses = list(chain.from_iterable(valid_running_losses))\n",
        "\n",
        "        train_epoch_idx = range(len(all_train_losses))\n",
        "        valid_epoch_idx = range(len(all_valid_losses))\n",
        "        # sns.lineplot(epoch_idx, all_losses)\n",
        "        sns.lineplot(train_epoch_idx, all_train_running_losses)\n",
        "        sns.lineplot(valid_epoch_idx, all_valid_running_losses)\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "6gwDC4mCKBkI"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare data\n",
        "train_data_loader = DataLoader(dataset, batch_size=5)\n",
        "val_data_loader = DataLoader(dataset, batch_size=5)\n"
      ],
      "metadata": {
        "id": "PTOxoy5JuRS8"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define our experimental setup; we use the same parameters for both MLP models\n",
        "input_size = dataset.input_text_len\n",
        "hidden_size = 100 # An arbitrary hyperparameter we define\n",
        "output_size = dataset.n_labels\n",
        "print(input_size)\n",
        "print(output_size)\n",
        "\n",
        "# Our experimental hyperparameters\n",
        "LEARNING_RATE = 1e-2\n",
        "\n",
        "loss_fn = nn.BCEWithLogitsLoss()"
      ],
      "metadata": {
        "id": "aJ6D5LtBjU1F",
        "outputId": "1efb1096-9d36-4a1c-f070-b2b7fd165b0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50\n",
            "19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mlp = MultiLayerPerceptron(input_size, hidden_size, output_size)\n",
        "optimizer = optim.SGD(mlp.parameters(), lr=LEARNING_RATE, momentum=0.9)\n",
        "\n",
        "\n",
        "mlp_trainer = MultiClassTrainer(mlp, optimizer, loss_fn)\n",
        "mlp_trainer.run_training(train_data_loader, val_data_loader, dataset.labels, n_epochs=2)"
      ],
      "metadata": {
        "id": "QI5hdBwbtzNA",
        "outputId": "c9a7e799-258d-40b6-9d8f-fe24c2b9ac2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 657
        }
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MultiLayerPerceptron(\n",
            "  (input_layer): Linear(in_features=50, out_features=100, bias=True)\n",
            "  (hidden_layer): Linear(in_features=100, out_features=19, bias=True)\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            ")\n",
            "SGD (\n",
            "Parameter Group 0\n",
            "    dampening: 0\n",
            "    foreach: None\n",
            "    lr: 0.01\n",
            "    maximize: False\n",
            "    momentum: 0.9\n",
            "    nesterov: False\n",
            "    weight_decay: 0\n",
            ")\n",
            "BCEWithLogitsLoss()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0it [00:00, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-67-6c76c0d09c43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmlp_trainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiClassTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmlp_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-64-290bac7edde2>\u001b[0m in \u001b[0;36mrun_training\u001b[0;34m(self, train_loader, valid_loader, labels, n_epochs)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m             \u001b[0mloss_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_loss_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m             \u001b[0mvalid_loss_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_running_loss_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-64-290bac7edde2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Always set gradient to 0 before computing it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# __call__ model() in this case: __call__ internally calls forward()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0;31m# [batch_size, num_classes]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-61-96ae99ddddb2>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_dropout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (250x100 and 50x100)"
          ]
        }
      ]
    }
  ]
}